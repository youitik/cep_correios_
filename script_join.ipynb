{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import bibliotecas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variaveis de entrada\n",
    "#['TABELA_FRETE_FINAL_BELO HORIZONTE','TABELA_FRETE_FINAL_NITEROI','TABELA_FRETE_FINAL_PORTO ALEGRE','TABELA_FRETE_FINAL_RIO DE JANEIRO','TABELA_FRETE_FINAL_SAO JOSE DOS CAMPOS','TABELA_FRETE_FINAL_UBA']\n",
    "# faltando BELO HORIZONTE RIO DE JANEIRO SAO JOSE DOS CAMPOS\n",
    "\n",
    "#arquivo_cidade = ['BELO HORIZONTE','RIO DE JANEIRO','NITEROI','PORTO ALEGRE','SAO JOSE DOS CAMPOS','UBA']\n",
    "arquivo_cidade = ['NITEROI','SAO JOSE DOS CAMPOS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "join sucesso - final\n",
      "linhas: 127926\n",
      "qtd nulos: 0\n",
      "NITEROI\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daniel.kabata\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\ipykernel_launcher.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "join sucesso - check\n",
      "linhas: 127926\n",
      "qtd nulos: 0\n",
      "NITEROI\n",
      "------------------------\n",
      "generating nice layout for NITEROI...\n",
      "------------------------\n",
      "join sucesso - colunado\n",
      "linhas: 5573\n",
      "qtd nulos: 0\n",
      "NITEROI\n",
      "------------------------\n",
      "------------------------\n",
      "join sucesso - colunado\n",
      "linhas: 5573\n",
      "qtd nulos: 0\n",
      "NITEROI\n",
      "------------------------\n",
      "------------------------\n",
      "join sucesso - colunado - prazos\n",
      "linhas: 5573\n",
      "qtd nulos: 0\n",
      "NITEROI\n",
      "------------------------\n",
      "------------------------\n",
      "join sucesso - final\n",
      "linhas: 127310\n",
      "qtd nulos: 0\n",
      "SAO JOSE DOS CAMPOS\n",
      "------------------------\n",
      "------------------------\n",
      "join sucesso - check\n",
      "linhas: 127310\n",
      "qtd nulos: 0\n",
      "SAO JOSE DOS CAMPOS\n",
      "------------------------\n",
      "generating nice layout for SAO JOSE DOS CAMPOS...\n",
      "------------------------\n",
      "join sucesso - colunado\n",
      "linhas: 5573\n",
      "qtd nulos: 0\n",
      "SAO JOSE DOS CAMPOS\n",
      "------------------------\n",
      "------------------------\n",
      "join sucesso - colunado\n",
      "linhas: 5573\n",
      "qtd nulos: 0\n",
      "SAO JOSE DOS CAMPOS\n",
      "------------------------\n",
      "------------------------\n",
      "join sucesso - colunado - prazos\n",
      "linhas: 5573\n",
      "qtd nulos: 0\n",
      "SAO JOSE DOS CAMPOS\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "# main()\n",
    "\n",
    "# join prazos\n",
    "for arquivo in arquivo_cidade:\n",
    "    df = join_prazos_final(arquivo)\n",
    "    df2 = join_prazos_check(arquivo)\n",
    "    gera_novo_formato(df, arquivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gera_novo_formato(df, arquivo_cidade):\n",
    "    \n",
    "    # import de arquivos\n",
    "    df_cidades = pd.read_csv(\"arquivos_origens/cidade_cep.csv\", sep = ';')\n",
    "    print 'generating nice layout for ' + arquivo_cidade + '...'\n",
    "    \n",
    "    # marca cep de origem e destino\n",
    "    ORIGEM_S = df['ORIGIN_ZIP_START'][1]\n",
    "    ORIGEM_E = df['ORIGIN_ZIP_END'][1]\n",
    "   \n",
    "    # cria pivot table com servico e peso como colunas\n",
    "    table = pd.pivot_table(df, values=['SHIPPING_COST'], index=['CEP_INICIAL', 'CEP_FINAL'],\n",
    "                columns=['SERVICE_UID','WEIGHT_START'], aggfunc=np.max)\n",
    "    \n",
    "    # tratamentos de index\n",
    "    # remove multiindex\n",
    "    table.columns = table.columns.map('{0[0]}_{0[1]}'.format)\n",
    "    \n",
    "    # reset index\n",
    "    table = table.reset_index()\n",
    "    \n",
    "    # cria colunas de origem\n",
    "    table['ORIGIN_ZIP_START'] = ORIGEM_S\n",
    "    table['ORIGIN_ZIP_END'] = ORIGEM_E\n",
    "    \n",
    "    # trata nomes de colunas\n",
    "    table.columns = ['CEP_INICIAL','CEP_FINAL','SHIPPING_COST_1_300G','SHIPPING_COST_1_500G','SHIPPING_COST_1_1KG','SHIPPING_COST_1_2KG','SHIPPING_COST_1_3KG','SHIPPING_COST_1_4KG','SHIPPING_COST_1_5KG','SHIPPING_COST_1_6KG','SHIPPING_COST_1_7KG','SHIPPING_COST_1_8KG','SHIPPING_COST_1_9KG','SHIPPING_COST_1_10KG','SHIPPING_COST_52_500G','SHIPPING_COST_52_1KG','SHIPPING_COST_52_2KG','SHIPPING_COST_52_3KG','SHIPPING_COST_52_4KG','SHIPPING_COST_52_5KG','SHIPPING_COST_52_6KG','SHIPPING_COST_52_7KG','SHIPPING_COST_52_8KG','SHIPPING_COST_52_9KG','SHIPPING_COST_52_10KG','ORIGIN_ZIP_START','ORIGIN_ZIP_END']\n",
    "    table_ordered = table[['ORIGIN_ZIP_START','ORIGIN_ZIP_END','CEP_INICIAL','CEP_FINAL','SHIPPING_COST_1_300G','SHIPPING_COST_1_500G','SHIPPING_COST_1_1KG','SHIPPING_COST_1_2KG','SHIPPING_COST_1_3KG','SHIPPING_COST_1_4KG','SHIPPING_COST_1_5KG','SHIPPING_COST_1_6KG','SHIPPING_COST_1_7KG','SHIPPING_COST_1_8KG','SHIPPING_COST_1_9KG','SHIPPING_COST_1_10KG','SHIPPING_COST_52_500G','SHIPPING_COST_52_1KG','SHIPPING_COST_52_2KG','SHIPPING_COST_52_3KG','SHIPPING_COST_52_4KG','SHIPPING_COST_52_5KG','SHIPPING_COST_52_6KG','SHIPPING_COST_52_7KG','SHIPPING_COST_52_8KG','SHIPPING_COST_52_9KG','SHIPPING_COST_52_10KG']]\n",
    "\n",
    "    # cria dataframes para merge origem destino localidades\n",
    "    df_cidades_destino = df_cidades[['CEP_INICIAL','UF','UF_LOCALIDADE','TIPO']]\n",
    "    df_cidades_destino.columns = ['CEP_INICIAL','UF_END','UF_CITY_END','TYPE_CITY_END']\n",
    "\n",
    "    df_cidades_origem = df_cidades[['CEP_INICIAL','UF','UF_LOCALIDADE','TIPO']]\n",
    "    df_cidades_origem.columns = ['ORIGIN_ZIP_START','UF_ORIGIN','UF_CITY_ORIGIN','TYPE_CITY_ORIGIN']\n",
    "\n",
    "    \n",
    "    # merge dataframes - destiny\n",
    "    try:\n",
    "        table_city = pd.merge(table_ordered,df_cidades_destino,on=['CEP_INICIAL'], how='left')\n",
    "        print '------------------------'\n",
    "        print 'join sucesso - colunado'\n",
    "        print 'linhas: ' + str(len(table_city))\n",
    "        print 'qtd nulos: ' + str(table_city['UF_END'].isnull().sum())\n",
    "        print arquivo_cidade\n",
    "        print '------------------------'\n",
    "\n",
    "    except:\n",
    "        print '------------------------'\n",
    "        print 'nao conseguiu fazer o join'\n",
    "        print arquivo_cidade\n",
    "        print '------------------------'\n",
    "\n",
    "\n",
    "    # merge dataframes - origim\n",
    "    try:\n",
    "        table_city_origin = pd.merge(table_city,df_cidades_origem,on=['ORIGIN_ZIP_START'], how='left')\n",
    "        print '------------------------'\n",
    "        print 'join sucesso - colunado'\n",
    "        print 'linhas: ' + str(len(table_city_origin))\n",
    "        print 'qtd nulos: ' + str(table_city_origin['UF_ORIGIN'].isnull().sum())\n",
    "        print arquivo_cidade\n",
    "        print '------------------------'\n",
    "\n",
    "    except:\n",
    "        print '------------------------'\n",
    "        print 'nao conseguiu fazer o join'\n",
    "        print arquivo_cidade\n",
    "        print '------------------------'\n",
    "\n",
    "        \n",
    "    # ordenacao tabela final\n",
    "    table_ordered_final = table_city_origin[['UF_ORIGIN','UF_CITY_ORIGIN','TYPE_CITY_ORIGIN','ORIGIN_ZIP_START','ORIGIN_ZIP_END','UF_END','UF_CITY_END','TYPE_CITY_END','CEP_INICIAL','CEP_FINAL','SHIPPING_COST_1_300G','SHIPPING_COST_1_500G','SHIPPING_COST_1_1KG','SHIPPING_COST_1_2KG','SHIPPING_COST_1_3KG','SHIPPING_COST_1_4KG','SHIPPING_COST_1_5KG','SHIPPING_COST_1_6KG','SHIPPING_COST_1_7KG','SHIPPING_COST_1_8KG','SHIPPING_COST_1_9KG','SHIPPING_COST_1_10KG','SHIPPING_COST_52_500G','SHIPPING_COST_52_1KG','SHIPPING_COST_52_2KG','SHIPPING_COST_52_3KG','SHIPPING_COST_52_4KG','SHIPPING_COST_52_5KG','SHIPPING_COST_52_6KG','SHIPPING_COST_52_7KG','SHIPPING_COST_52_8KG','SHIPPING_COST_52_9KG','SHIPPING_COST_52_10KG']]\n",
    "\n",
    "    # inclui prazos\n",
    "    df_final_export = layout_colunado_prazos(table_ordered_final,arquivo_cidade)\n",
    "    \n",
    "    df_final_export.to_csv('arquivos_gerados_colunados/TABELA_FRETE_FINAL_'+arquivo_cidade+'_COLUNADO.csv',index=False,sep=';')\n",
    "    \n",
    "    return table_ordered_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_prazos_check(arquivo_cidade):\n",
    "    \n",
    "    #import tabelas\n",
    "    df_final = pd.read_csv(\"arquivos_origens/TABELA_FRETE_CHECK_\"+arquivo_cidade+\".csv\",sep = ';')\n",
    "    df_prazos = pd.read_csv(\"arquivos_origens/ARQUIVO_PRAZOS.csv\", sep = ';')\n",
    "\n",
    "    # renomeia colunas prazo\n",
    "    df_prazos.columns = ['CEP_INICIAL', 'ORIGIN_ZIP_START','SERVICE','SERVICE_UID','PRAZO_CORREIOS','DELIVERY_TIME_START','DELIVERY_TIME_END','DELIVERY_TIME_SCALE']\n",
    "    \n",
    "      \n",
    "    # altera tipos das colunas para fazer merge\n",
    "    df_final['ORIGIN_ZIP_START'] = df_final['ORIGIN_ZIP_START'].astype('int64')\n",
    "    df_prazos['ORIGIN_ZIP_START'] = df_prazos['ORIGIN_ZIP_START'].astype('int64')\n",
    "\n",
    "    df_final['CEP_INICIAL'] = df_final['CEP_INICIAL'].astype('int64')\n",
    "    df_prazos['CEP_INICIAL'] = df_prazos['CEP_INICIAL'].astype('int64')\n",
    "\n",
    "    df_final['SERVICE_UID'] = df_final['SERVICE_UID'].astype('int64')\n",
    "    df_prazos['SERVICE_UID'] = df_prazos['SERVICE_UID'].astype('int64')\n",
    "    \n",
    "    \n",
    "    # fixes #########################################################################################\n",
    "    \n",
    "    # onde service_uid = 51 entao 52\n",
    "    df_final['SERVICE_UID'] = np.where(df_final['SERVICE_UID']==51, 52, df_final['SERVICE_UID'])\n",
    "    df_prazos['SERVICE_UID'] = np.where(df_prazos['SERVICE_UID']==51, 52, df_prazos['SERVICE_UID'])\n",
    "\n",
    "    # origem uba = 36500000\n",
    "    df_prazos['ORIGIN_ZIP_START'] = np.where(df_prazos['ORIGIN_ZIP_START']==36500001, 36500000, df_prazos['ORIGIN_ZIP_START'])\n",
    "\n",
    "    # weigth start nao arredondado\n",
    "    df_final['WEIGHT_START'] = np.where(df_final['WEIGHT_START']==3.0010000000000003, 3.001, df_final['WEIGHT_START'])\n",
    "\n",
    "    # fim fixes #########################################################################################\n",
    "    \n",
    "    # merge dataframes\n",
    "    try:\n",
    "        df_final_com_prazos = pd.merge(df_final,df_prazos,on=['ORIGIN_ZIP_START','CEP_INICIAL','SERVICE_UID'], how='left')\n",
    "        print '------------------------'\n",
    "        print 'join sucesso - check'\n",
    "        print 'linhas: ' + str(len(df_final_com_prazos))\n",
    "        print 'qtd nulos: ' + str(df_final_com_prazos['DELIVERY_TIME_START'].isnull().sum())\n",
    "        print arquivo_cidade\n",
    "        print '------------------------'\n",
    "    \n",
    "    except:\n",
    "        print '------------------------'\n",
    "        print 'nao conseguiu fazer o join'\n",
    "        print arquivo_cidade\n",
    "        print '------------------------'\n",
    "        \n",
    "    #export dataframe\n",
    "    df_final_com_prazos.to_csv('arquivos_gerados_check/TABELA_FRETE_CHECK_'+arquivo_cidade+'_COM_PRAZOS.csv',index=False,sep=';')\n",
    "    \n",
    "    return df_final_com_prazos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_prazos_final(arquivo_cidade):\n",
    "    \n",
    "    #import tabelas\n",
    "    df_final = pd.read_csv(\"arquivos_origens/TABELA_FRETE_FINAL_\"+arquivo_cidade+\".csv\",sep = ';')\n",
    "    df_prazos = pd.read_csv(\"arquivos_origens/ARQUIVO_PRAZOS.csv\", sep = ';')\n",
    "\n",
    "    # renomeia colunas prazo\n",
    "    df_prazos.columns = ['CEP_INICIAL', 'ORIGIN_ZIP_START','SERVICE','SERVICE_UID','PRAZO_CORREIOS','DELIVERY_TIME_START','DELIVERY_TIME_END','DELIVERY_TIME_SCALE']\n",
    "    \n",
    "      \n",
    "    # altera tipos das colunas para fazer merge\n",
    "    df_final['ORIGIN_ZIP_START'] = df_final['ORIGIN_ZIP_START'].astype('int64')\n",
    "    df_prazos['ORIGIN_ZIP_START'] = df_prazos['ORIGIN_ZIP_START'].astype('int64')\n",
    "\n",
    "    df_final['CEP_INICIAL'] = df_final['CEP_INICIAL'].astype('int64')\n",
    "    df_prazos['CEP_INICIAL'] = df_prazos['CEP_INICIAL'].astype('int64')\n",
    "\n",
    "    df_final['SERVICE_UID'] = df_final['SERVICE_UID'].astype('int64')\n",
    "    df_prazos['SERVICE_UID'] = df_prazos['SERVICE_UID'].astype('int64')\n",
    "    \n",
    "    \n",
    "    # fixes #########################################################################################\n",
    "    \n",
    "    # onde service_uid = 52 entao 51\n",
    "    df_final['SERVICE_UID'] = np.where(df_final['SERVICE_UID']==51, 52, df_final['SERVICE_UID'])\n",
    "    df_prazos['SERVICE_UID'] = np.where(df_prazos['SERVICE_UID']==51, 52, df_prazos['SERVICE_UID'])\n",
    "\n",
    "    # origem uba = 36500000\n",
    "    df_prazos['ORIGIN_ZIP_START'] = np.where(df_prazos['ORIGIN_ZIP_START']==36500001, 36500000, df_prazos['ORIGIN_ZIP_START'])\n",
    "\n",
    "    # weigth start nao arredondado e days = dias\n",
    "    df_final['WEIGHT_START'] = np.where(df_final['WEIGHT_START']==3.0010000000000003, 3.001, df_final['WEIGHT_START'])\n",
    "    df_final['WEIGHT_END'] = np.where(df_final['WEIGHT_END']==300, 0.300, df_final['WEIGHT_END'])\n",
    "    df_final['WEIGHT_END'] = np.where(df_final['WEIGHT_END']==500, 0.500, df_final['WEIGHT_END'])\n",
    "    df_prazos['DELIVERY_TIME_SCALE'] = np.where(df_prazos['DELIVERY_TIME_SCALE']=='DAYS', 'Dias', df_prazos['DELIVERY_TIME_SCALE'])\n",
    "\n",
    "    # fim fixes #########################################################################################\n",
    "    \n",
    "    # merge dataframes\n",
    "    try:\n",
    "        df_final_com_prazos = pd.merge(df_final,df_prazos,on=['ORIGIN_ZIP_START','CEP_INICIAL','SERVICE_UID'], how='left')\n",
    "        print '------------------------'\n",
    "        print 'join sucesso - final'\n",
    "        print 'linhas: ' + str(len(df_final_com_prazos))\n",
    "        print 'qtd nulos: ' + str(df_final_com_prazos['DELIVERY_TIME_START'].isnull().sum())\n",
    "        print arquivo_cidade\n",
    "        print '------------------------'\n",
    "    \n",
    "    except:\n",
    "        print '------------------------'\n",
    "        print 'nao conseguiu fazer o join'\n",
    "        print arquivo_cidade\n",
    "        print '------------------------'\n",
    "        \n",
    "    #export dataframe\n",
    "    #df_final_com_prazos.rename(columns={'CEP_INICIAL':'ZIP_START','CEP_FINAL':'ZIP_END'},inplace=True)\n",
    "    df_final_layout = df_final_com_prazos[['ORIGIN_ZIP_START','ORIGIN_ZIP_END','CARRIER_UID','SERVICE_UID','CEP_INICIAL','CEP_FINAL','WEIGHT_START','WEIGHT_END','SHIPPING_COST','DELIVERY_TIME_START','DELIVERY_TIME_END','DELIVERY_TIME_SCALE']]\n",
    "    df_final_layout['SHIPPING_COST'] = df_final_layout['SHIPPING_COST'].str.replace(',','.')\n",
    "    #df_final_layout['DELIVERY_TIME_SCALE'] = df_final_layout['DELIVERY_TIME_SCALE'].map({'DAYS':'Dias'})\n",
    "    df_final_layout.to_csv('arquivos_gerados/TABELA_FRETE_FINAL_'+arquivo_cidade+'_COM_PRAZOS.csv',index=False,sep=';')\n",
    "    \n",
    "    return df_final_com_prazos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_prazos_colunado(arquivo_cidade, df_final):\n",
    "    \n",
    "    #import tabelas\n",
    "    df_final = df_final\n",
    "    df_prazos = pd.read_csv(\"arquivos_origens/ARQUIVO_PRAZOS.csv\", sep = ';')\n",
    "\n",
    "    # renomeia colunas prazo\n",
    "    df_prazos.columns = ['CEP_INICIAL', 'ORIGIN_ZIP_START','SERVICE','SERVICE_UID','PRAZO_CORREIOS','DELIVERY_TIME_START','DELIVERY_TIME_END','DELIVERY_TIME_SCALE']\n",
    "    \n",
    "      \n",
    "    # altera tipos das colunas para fazer merge\n",
    "    df_final['ORIGIN_ZIP_START'] = df_final['ORIGIN_ZIP_START'].astype('int64')\n",
    "    df_prazos['ORIGIN_ZIP_START'] = df_prazos['ORIGIN_ZIP_START'].astype('int64')\n",
    "\n",
    "    df_final['CEP_INICIAL'] = df_final['CEP_INICIAL'].astype('int64')\n",
    "    df_prazos['CEP_INICIAL'] = df_prazos['CEP_INICIAL'].astype('int64')\n",
    "\n",
    "        \n",
    "    # fixes #########################################################################################\n",
    "    \n",
    "    # onde service_uid = 52 entao 51\n",
    "    df_prazos['SERVICE_UID'] = np.where(df_prazos['SERVICE_UID']==51, 52, df_prazos['SERVICE_UID'])\n",
    "\n",
    "    # origem uba = 36500000\n",
    "    df_prazos['ORIGIN_ZIP_START'] = np.where(df_prazos['ORIGIN_ZIP_START']==36500001, 36500000, df_prazos['ORIGIN_ZIP_START'])\n",
    "\n",
    "    # weigth start nao arredondado\n",
    "    df_final['WEIGHT_START'] = np.where(df_final['WEIGHT_START']==3.0010000000000003, 3.001, df_final['WEIGHT_START'])\n",
    "\n",
    "\n",
    "    # fim fixes #########################################################################################\n",
    "    \n",
    "    # merge dataframes\n",
    "    try:\n",
    "        df_final_com_prazos = pd.merge(df_final,df_prazos,on=['ORIGIN_ZIP_START','CEP_INICIAL','SERVICE_UID'], how='left')\n",
    "        print '------------------------'\n",
    "        print 'join sucesso - final'\n",
    "        print 'linhas: ' + str(len(df_final_com_prazos))\n",
    "        print 'qtd nulos: ' + str(df_final_com_prazos['DELIVERY_TIME_START'].isnull().sum())\n",
    "        print arquivo_cidade\n",
    "        print '------------------------'\n",
    "    \n",
    "    except:\n",
    "        print '------------------------'\n",
    "        print 'nao conseguiu fazer o join'\n",
    "        print arquivo_cidade\n",
    "        print '------------------------'\n",
    "        \n",
    "    #export dataframe\n",
    "    df_final_com_prazos.to_csv('arquivos_gerados/TABELA_FRETE_FINAL_'+arquivo_cidade+'_COM_PRAZOS.csv',index=False,sep=';')\n",
    "    return df_final_com_prazos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layout_colunado_prazos(table_ordered_final,arquivo_cidade):\n",
    "    \n",
    "    # importa prazos\n",
    "    df_prazos = pd.read_csv(\"arquivos_origens/ARQUIVO_PRAZOS.csv\", sep = ';')\n",
    "    df_prazos.columns = ['CEP_INICIAL', 'ORIGIN_ZIP_START','SERVICE','SERVICE_UID','PRAZO_CORREIOS','DELIVERY_TIME_START','DELIVERY_TIME_END','DELIVERY_TIME_SCALE']\n",
    "    \n",
    "    # trata prazos\n",
    "    df_prazos['SERVICE_UID'] = np.where(df_prazos['SERVICE_UID']==51, 52, df_prazos['SERVICE_UID'])\n",
    "    \n",
    "    # cria pivot table com prazos\n",
    "    table = pd.pivot_table(df_prazos, values=['DELIVERY_TIME_START','DELIVERY_TIME_END'], index=['CEP_INICIAL','ORIGIN_ZIP_START'],\n",
    "                columns=['SERVICE_UID'], aggfunc=np.max)\n",
    "    \n",
    "    # tratamentos de index\n",
    "    # remove multiindex\n",
    "    table.columns = table.columns.map('{0[0]}_{0[1]}'.format)\n",
    "\n",
    "    # reset index\n",
    "    table = table.reset_index()\n",
    "\n",
    "    # trata nomes de colunas\n",
    "    table.columns = ['CEP_INICIAL','ORIGIN_ZIP_START','DELIVERY_TIME_START_52','DELIVERY_TIME_END_52','DELIVERY_TIME_START_1','DELIVERY_TIME_END_1']\n",
    "    table_ordered = table[['ORIGIN_ZIP_START','CEP_INICIAL','DELIVERY_TIME_START_1','DELIVERY_TIME_END_1','DELIVERY_TIME_START_52','DELIVERY_TIME_END_52']]\n",
    "    \n",
    "    # merge dataframes - origim\n",
    "    try:\n",
    "        table_final_layout = pd.merge(table_ordered_final,table_ordered,on=['ORIGIN_ZIP_START','CEP_INICIAL'], how='left')\n",
    "        print '------------------------'\n",
    "        print 'join sucesso - colunado - prazos'\n",
    "        print 'linhas: ' + str(len(table_final_layout))\n",
    "        print 'qtd nulos: ' + str(table_final_layout['DELIVERY_TIME_START_1'].isnull().sum())\n",
    "        print arquivo_cidade\n",
    "        print '------------------------'\n",
    "\n",
    "    except:\n",
    "        print '------------------------'\n",
    "        print 'nao conseguiu fazer o join - prazos '\n",
    "        print arquivo_cidade\n",
    "        print '------------------------'\n",
    "\n",
    "\n",
    "    # ordenacao tabela final\n",
    "    table_final_layout_ordered = table_final_layout[['UF_ORIGIN','UF_CITY_ORIGIN','TYPE_CITY_ORIGIN','ORIGIN_ZIP_START','ORIGIN_ZIP_END','UF_END','UF_CITY_END','TYPE_CITY_END','CEP_INICIAL','CEP_FINAL','SHIPPING_COST_1_300G','SHIPPING_COST_1_500G','SHIPPING_COST_1_1KG','SHIPPING_COST_1_2KG','SHIPPING_COST_1_3KG','SHIPPING_COST_1_4KG','SHIPPING_COST_1_5KG','SHIPPING_COST_1_6KG','SHIPPING_COST_1_7KG','SHIPPING_COST_1_8KG','SHIPPING_COST_1_9KG','SHIPPING_COST_1_10KG','SHIPPING_COST_52_500G','SHIPPING_COST_52_1KG','SHIPPING_COST_52_2KG','SHIPPING_COST_52_3KG','SHIPPING_COST_52_4KG','SHIPPING_COST_52_5KG','SHIPPING_COST_52_6KG','SHIPPING_COST_52_7KG','SHIPPING_COST_52_8KG','SHIPPING_COST_52_9KG','SHIPPING_COST_52_10KG','DELIVERY_TIME_START_1','DELIVERY_TIME_END_1','DELIVERY_TIME_START_52','DELIVERY_TIME_END_52']]\n",
    "\n",
    "    # trata datas sem PAC\n",
    "    table_final_layout_ordered['DELIVERY_TIME_START_52'] = np.where(table_final_layout_ordered['SHIPPING_COST_52_500G'].isnull(),'',table_final_layout_ordered['DELIVERY_TIME_START_52'])\n",
    "    table_final_layout_ordered['DELIVERY_TIME_END_52'] = np.where(table_final_layout_ordered['SHIPPING_COST_52_500G'].isnull(),'',table_final_layout_ordered['DELIVERY_TIME_END_52'])\n",
    "\n",
    "    \n",
    "    #table_final_layout_ordered.to_csv('arquivos_gerados_colunados/TABELA_FRETE_FINAL_'+arquivo_cidade+'_COLUNADO.csv',index=False,sep=';')\n",
    "\n",
    "    return table_final_layout_ordered\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
